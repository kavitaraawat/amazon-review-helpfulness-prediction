{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import math\n",
    "import nltk\n",
    "import string \n",
    "import scipy\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn.metrics as skmetrics\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import ensemble\n",
    "from nltk.corpus import cmudict \n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read from gzip file\n",
    "def read_gzip(filename):\n",
    "    for line in gzip.open(filename):\n",
    "        yield eval(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get (positive/negative) opinion words from corpus\n",
    "def get_opinion_words(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            yield line\n",
    "            \n",
    "positive_words = set()\n",
    "negative_words = set()\n",
    "\n",
    "for pword in get_opinion_words('positive-words.txt'):\n",
    "    positive_words.add(pword[:-2])\n",
    "    \n",
    "for nword in get_opinion_words('negative-words.txt'):\n",
    "    negative_words.add(nword[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute number of syllables for a given word\n",
    "d = cmudict.dict() \n",
    "def nsyl(word):\n",
    "    max_syl = 0\n",
    "    if word.lower() in d:\n",
    "        for syl_group in d[word.lower()]:\n",
    "            tot_syl = 0\n",
    "            for syl in syl_group:\n",
    "                if str(syl[-1]).isdigit():\n",
    "                    tot_syl += 1\n",
    "            max_syl = max(max_syl,tot_syl)\n",
    "    return max_syl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pruning\n",
    "\n",
    "- Removed data points with greater than 150 votes as the test data has very few data points in that range. \n",
    "- Prepared two separate datasete one for highly votes reviews (>10) and one for reviews that recieved low votes (<10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "high_dataset = []\n",
    "for line in read_gzip(\"train.json.gz\"):\n",
    "    if (line['helpful'])['outOf'] > 10 and (line['helpful'])['outOf'] <150:\n",
    "        high_dataset.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "low_dataset = []\n",
    "for line in read_gzip(\"train.json.gz\"):\n",
    "    if (line['helpful'])['outOf'] <= 80 and (line['helpful'])['outOf'] > 1 :\n",
    "        low_dataset.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute USER specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_user_ratings_dict = defaultdict(list)\n",
    "train_all_helpful_review = []\n",
    "train_user_helpful_review_dict = defaultdict(list)\n",
    "train_user_review_content_dict = defaultdict(list)\n",
    "\n",
    "for data_point in read_gzip(\"train.json.gz\"):   \n",
    "    if (data_point['helpful'])['outOf'] == 0:\n",
    "        continue\n",
    "        \n",
    "    train_user_ratings_dict[data_point['reviewerID']].append(data_point['rating'])\n",
    "    train_all_helpful_review.append(data_point['helpful'])\n",
    "    train_user_helpful_review_dict[data_point['reviewerID']].append(data_point['helpful'])    \n",
    "    train_user_review_content_dict[data_point['reviewerID']].append(data_point['reviewText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute ITEM specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Initialize ITEM specific data structures\n",
    "train_items_ratings_dict = defaultdict(list)\n",
    "train_user_purchased_items_dict = defaultdict(list)\n",
    "for line in read_gzip(\"train.json.gz\"):\n",
    "    user = line['reviewerID']\n",
    "    item = line['itemID']\n",
    "    train_user_purchased_items_dict[user].append(item)\n",
    "    train_items_ratings_dict[item].append(line['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ratings computed for 19913 items\n",
      "Number of reviews computed for 19913 items\n"
     ]
    }
   ],
   "source": [
    "train_average_items_ratings_dict = {}\n",
    "train_item_review_count = {}\n",
    "for item in train_items_ratings_dict:\n",
    "    train_average_items_ratings_dict[item] = np.mean(train_items_ratings_dict[item])\n",
    "    train_item_review_count[item] = len(train_items_ratings_dict[item])\n",
    "print \"Average ratings computed for \" + str(len(train_average_items_ratings_dict.values())) + \" items\"\n",
    "print \"Number of reviews computed for \" + str(len(train_item_review_count.values())) + \" items\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute OVERALL average helpfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Helpfulness : 0.851972088653\n"
     ]
    }
   ],
   "source": [
    "global_average_helpfulness = sum([x['nHelpful'] for x in train_all_helpful_review]) * 1.0 / sum([\n",
    "        x['outOf'] for x in train_all_helpful_review])\n",
    "print (\"Average Helpfulness : %s\" % global_average_helpfulness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USER SPECIFIC FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_user_review_experience_count_feature(user_review_text_dict):\n",
    "    user_review_experience = {}\n",
    "    # Compute number of reviews given by a user\n",
    "    for user in user_review_text_dict:\n",
    "        user_review_experience[user] = len(user_review_text_dict[user])\n",
    "    return user_review_experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_average_ratings_feature(user_ratings_dict, train_global_average_ratings):\n",
    "    user_average_ratings = {}\n",
    "    # Compute average ratings given by user or fill with global average ratings\n",
    "    for user in user_ratings_dict:\n",
    "        total_user_ratings = len(train_user_ratings_dict[user])\n",
    "        if total_user_ratings > 0:\n",
    "            user_average_ratings[user] = sum(train_user_ratings_dict[user]) * 1.0/total_user_ratings\n",
    "        else:\n",
    "            user_average_ratings[user] = train_global_average_ratings\n",
    "    return user_average_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_average_helpfulness_feature(train_user_helpful_review_dict, train_global_average_helpfulness):\n",
    "    user_average_helpfulness = {}\n",
    "    # Compute average helpfulness of users or fill with global average helpfulness values\n",
    "    for user in train_user_helpful_review_dict:\n",
    "        total_user_helpful_review = sum([x['outOf'] for x in train_user_helpful_review_dict[user]])\n",
    "        if total_user_helpful_review > 0:\n",
    "            user_average_helpfulness[user] = sum(\n",
    "                [x['nHelpful'] for x in train_user_helpful_review_dict[user]]) * 1.0 / total_user_helpful_review\n",
    "        else:\n",
    "            user_average_helpfulness[user] = train_global_average_helpfulness\n",
    "\n",
    "    return user_average_helpfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_rating_deviation_feature(user_ratings_dict, average_items_ratings_dict, user_purchased_items_dict):\n",
    "    user_rating_deviation = {}\n",
    "    for user in user_ratings_dict:\n",
    "        user_rating_deviation[user] = np.mean([(user_ratings_dict[user] - average_items_ratings_dict[item])**2 \n",
    "         for item in user_purchased_items_dict[user] if item in average_items_ratings_dict])\n",
    "    return user_rating_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted user review experience for 29005 users\n"
     ]
    }
   ],
   "source": [
    "# FEATURE 14 : USER REVIEW EXPERIENCE \n",
    "train_user_review_experience = get_user_review_experience_count_feature(train_user_review_content_dict)\n",
    "print \"Extracted user review experience for \" + str(len(train_user_review_experience.values())) + \" users\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET DEPENDENT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_average_helpfulness(dataset):\n",
    "    data_average_helpfulness = []\n",
    "    for data_point in dataset:\n",
    "        data_average_helpfulness.append(global_average_helpfulness)\n",
    "    return data_average_helpfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rating(dataset):\n",
    "    data_ratings = []\n",
    "    for data_point in dataset:\n",
    "        data_ratings.append(data_point['rating'])\n",
    "    return data_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_square_rating(dataset):\n",
    "    data_ratings = []\n",
    "    for data_point in dataset:\n",
    "        data_ratings.append(data_point['rating']**2)\n",
    "    return data_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_log_ratings(dataset):\n",
    "    data_ratings = []\n",
    "    for data_point in dataset:\n",
    "        data_ratings.append(data_point['rating'])\n",
    "    data_ratings = np.array(data_ratings)\n",
    "    return np.log(data_ratings.max() + 1 - data_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_helpfulness_votes(dataset):\n",
    "    data_helpfulness_votes = []\n",
    "    for data_point in dataset:\n",
    "        votes_json = data_point['helpful']\n",
    "        data_helpfulness_votes.append(np.log(votes_json['outOf'] + 1))\n",
    "    return data_helpfulness_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_review_word_count(dataset):\n",
    "    data_review_word_count = []\n",
    "    for data_point in dataset:\n",
    "        data_review_word_count.append(np.log(len(data_point['reviewText'].lower().split())+1))\n",
    "    return data_review_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_count(dataset):\n",
    "    data_review_sentence_count = []\n",
    "    for data_point in dataset:\n",
    "        data_review_sentence_count.append(np.log(len(data_point['reviewText'].lower().split('.'))+1))\n",
    "    return data_review_sentence_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_review_allcaps_count(dataset):\n",
    "    data_review_word_allcaps_count = []\n",
    "    for data_point in dataset:\n",
    "        data_review_word_allcaps_count.append(np.log(len([\n",
    "                        word for word in data_point['reviewText'].split() if word.isupper()])+1))\n",
    "    return data_review_word_allcaps_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_review_char_count(dataset):\n",
    "    data_review_char_count = []\n",
    "    for data_point in dataset:\n",
    "        data_review_char_count.append(np.log(sum([len(word) for word in data_point['reviewText'].lower().split()])+1))\n",
    "    return data_review_char_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_review_specialchar_count(dataset):\n",
    "    data_review_specialchar_count = []\n",
    "    for data_point in dataset:\n",
    "        data_review_specialchar_count.append(len([word for word in data_point['reviewText'].lower().split() \n",
    "                                           if \"!\" in word or \"?\" in word ]))\n",
    "    return data_review_specialchar_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_item_rating_deviation(dataset):\n",
    "    data_item_rating_deviation = []\n",
    "    for data_point in dataset:\n",
    "        data_item_rating_deviation.append(np.abs(data_point['rating'] - train_average_items_ratings_dict[data_point['itemID']]))\n",
    "    return data_item_rating_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_flesch_reading_ease_score(dataset):\n",
    "    data_review_flesch_reading_score = []\n",
    "    for data_point in dataset:\n",
    "        total_words = len(data_point['reviewText'].lower().split())\n",
    "        total_sent = len(data_point['reviewText'].lower().split('.'))\n",
    "        total_syllable = sum([nsyl(word) for word in data_point['reviewText'].lower().split()])\n",
    "        data_review_flesch_reading_score.append(206.835 - (1.015*(total_words* 1.0/(1+total_sent))) - \\\n",
    "                                                (84.6*(total_syllable * 1.0/(1+total_words))))        \n",
    "    return data_review_flesch_reading_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_readability_index(dataset):\n",
    "    data_review_readability_index = []\n",
    "    for data_point in dataset:\n",
    "        total_char = sum([len(word) for word in data_point['reviewText'].lower().split()])\n",
    "        total_words = len(data_point['reviewText'].lower().split())\n",
    "        total_sent = len(data_point['reviewText'].lower().split('.'))\n",
    "        data_review_readability_index.append((4.71*(total_char*1.0/(1+total_words))) + \n",
    "                                            (0.5*(total_words*1.0/(1+total_sent))) - 21.43)\n",
    "    return data_review_readability_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_summary_word_count(dataset):\n",
    "    data_summary_word_count = []\n",
    "    for data_point in dataset:\n",
    "        data_summary_word_count.append(len([word for word in data_point['summary'].lower().split()]))\n",
    "    return data_summary_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_summary_char_count(dataset):\n",
    "    data_summary_char_count = []\n",
    "    for data_point in dataset:\n",
    "        data_summary_char_count.append(np.log(sum([len(word) for word in data_point['summary'].lower().split()])+1))\n",
    "    return data_summary_char_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_summary_allcaps_count(dataset):\n",
    "    data_summary_word_allcaps_count = []\n",
    "    for data_point in dataset:\n",
    "        data_summary_word_allcaps_count.append(sum([1 for word in data_point['summary'].split() if word.isupper()]))\n",
    "    return data_summary_word_allcaps_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_summary_specialchar_count(dataset):\n",
    "    data_summary_specialchar_count = []\n",
    "    for data_point in dataset:\n",
    "        data_summary_specialchar_count.append(len([word for word in data_point['summary'].lower().split() \n",
    "                                           if \"!\" in word or \"?\" in word ]))\n",
    "    return data_summary_specialchar_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_review_sentiment_score(dataset):\n",
    "    data_review_pos_sentiment = []\n",
    "    data_review_neg_sentiment = []\n",
    "    data_review_obj_sentiment = []\n",
    "    \n",
    "    for data_point in dataset:\n",
    "        review_text = data_point['reviewText'].lower().split()\n",
    "        data_review_pos_sentiment.append(sum([\n",
    "                    sum([x.pos_score() for x in swn.senti_synsets(word.lower())]) for word in review_text]))\n",
    "        data_review_neg_sentiment.append(sum([\n",
    "                    sum([x.neg_score() for x in swn.senti_synsets(word.lower())]) for word in review_text]))\n",
    "        data_review_obj_sentiment.append(sum([\n",
    "                    sum([x.obj_score() for x in swn.senti_synsets(word.lower())]) for word in review_text]))\n",
    "    return data_review_pos_sentiment, data_review_neg_sentiment, data_review_obj_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_summary_sentiment_score(dataset):\n",
    "    data_summary_pos_sentiment = []\n",
    "    data_summary_neg_sentiment = []\n",
    "    data_summary_obj_sentiment = []\n",
    "    for data_point in dataset:\n",
    "        summary_text = data_point['summary'].lower().split()\n",
    "        data_summary_pos_sentiment.append(sum([\n",
    "                    sum([x.pos_score() for x in swn.senti_synsets(word.lower())]) for word in summary_text]))\n",
    "        data_summary_neg_sentiment.append(sum([\n",
    "                    sum([x.neg_score() for x in swn.senti_synsets(word.lower())]) for word in summary_text]))\n",
    "        data_summary_obj_sentiment.append(sum([\n",
    "                    sum([x.obj_score() for x in swn.senti_synsets(word.lower())]) for word in summary_text]))        \n",
    "        \n",
    "    return data_summary_pos_sentiment, data_summary_neg_sentiment, data_summary_obj_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_review_experience(dataset):\n",
    "    data_user_review_experience = []\n",
    "    for data_point in dataset:\n",
    "        if data_point['reviewerID'] in train_user_review_experience:\n",
    "            data_user_review_experience.append(np.log(train_user_review_experience[data_point['reviewerID']]+1))\n",
    "        else:\n",
    "            data_user_review_experience.append(0)\n",
    "    return data_user_review_experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_price(dataset):\n",
    "    all_price = []\n",
    "    for data_point in dataset:\n",
    "        if 'price' in data_point: \n",
    "            if not math.isnan(data_point['price']) :\n",
    "                all_price.append(data_point['price'])\n",
    "    global_average_price = np.mean(np.array(all_price))\n",
    "    max_price = np.max(np.array(all_price))\n",
    "    data_price = []\n",
    "    for data_point in dataset:\n",
    "        if ('price' in data_point) and (not math.isnan(data_point['price'])):\n",
    "            data_price.append(np.cbrt(data_point['price']))\n",
    "        else:\n",
    "            data_price.append(np.cbrt(global_average_price))\n",
    "    return data_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_category_id(dataset):\n",
    "    category_0 = []\n",
    "    category_1 = []\n",
    "    category_2 = []\n",
    "    category_3 = []\n",
    "    category_4 = []\n",
    "\n",
    "    for data_point in dataset:\n",
    "        cat_id = data_point['categoryID']\n",
    "        if cat_id == 0:\n",
    "            category_0.append(1)\n",
    "            category_1.append(0)\n",
    "            category_2.append(0)\n",
    "            category_3.append(0)\n",
    "            category_4.append(0)\n",
    "        if cat_id == 1:\n",
    "            category_0.append(0)\n",
    "            category_1.append(1)\n",
    "            category_2.append(0)\n",
    "            category_3.append(0)\n",
    "            category_4.append(0)\n",
    "        if cat_id == 2:\n",
    "            category_0.append(0)\n",
    "            category_1.append(0)\n",
    "            category_2.append(1)\n",
    "            category_3.append(0)\n",
    "            category_4.append(0)\n",
    "        if cat_id == 3:\n",
    "            category_0.append(0)\n",
    "            category_1.append(0)\n",
    "            category_2.append(0)\n",
    "            category_3.append(1)\n",
    "            category_4.append(0)\n",
    "        if cat_id == 4:\n",
    "            category_0.append(0)\n",
    "            category_1.append(0)\n",
    "            category_2.append(0)\n",
    "            category_3.append(0)\n",
    "            category_4.append(1)\n",
    "    return category_0, category_1, category_2, category_3, category_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tag_category(dataset):\n",
    "    man = []\n",
    "    woman = []\n",
    "    boy = []\n",
    "    girl = []\n",
    "    for data_point in dataset:\n",
    "        categories = data_point['categories']\n",
    "        man_sum = 0\n",
    "        wom_sum = 0\n",
    "        boy_sum = 0\n",
    "        girl_sum = 0            \n",
    "        man.append(sum([sum([sum([1 if elem.lower()=='men' else 0 for elem in cat]) \n",
    "                             for cat in data_point['categories']])>0]))\n",
    "        woman.append(sum([sum([sum([1 if elem.lower()=='women' else 0 for elem in cat]) \n",
    "                               for cat in data_point['categories']])>0]))\n",
    "        boy.append(sum([sum([sum([1 if elem.lower()=='boys' else 0 for elem in cat]) \n",
    "                             for cat in data_point['categories']])>0]))\n",
    "        girl.append(sum([sum([sum([1 if elem.lower()=='girls' else 0 for elem in cat]) \n",
    "                              for cat in data_point['categories']])>0]))    \n",
    "    return man, woman, boy, girl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_adult_category(dataset):\n",
    "    adult = []\n",
    "    kid = []\n",
    "\n",
    "    for data_point in dataset:\n",
    "        categories = data_point['categories']           \n",
    "        adult.append(sum([sum([sum([1 if elem.lower()=='men' or elem.lower()=='women' else 0 for elem in cat]) \n",
    "                               for cat in data_point['categories']])>0]))\n",
    "        kid.append(sum([sum([sum([1 if elem.lower()=='boys' or elem.lower()=='girls' else 0 for elem in cat]) \n",
    "                             for cat in data_point['categories']])>0]))\n",
    "\n",
    "    return adult, kid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sex_category(dataset):\n",
    "    male = []\n",
    "    female = []\n",
    "\n",
    "    for data_point in dataset:\n",
    "        categories = data_point['categories']           \n",
    "        male.append(sum([sum([sum([1 if elem.lower()=='men' or elem.lower()=='boys' else 0 for elem in cat]) \n",
    "                              for cat in data_point['categories']])>0]))\n",
    "        female.append(sum([sum([sum([1 if elem.lower()=='women' or elem.lower()=='girls' else 0 for elem in cat]) \n",
    "                                for cat in data_point['categories']])>0]))\n",
    "\n",
    "    return male, female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_review_year(dataset):\n",
    "    review_years = []\n",
    "    for data_point in dataset:\n",
    "        year = int(data_point['reviewTime'].split(',')[-1].strip())\n",
    "        review_years.append(year - 2002)\n",
    "    return review_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_average_ratings(dataset):\n",
    "    user_average_ratings = []\n",
    "    for data_point in dataset:\n",
    "        if data_point['reviewerID'] in train_user_ratings:\n",
    "            user_average_ratings.append(train_user_ratings[data_point['reviewerID']])\n",
    "        else:\n",
    "            user_average_ratings.append(train_global_average_ratings)\n",
    "    return user_average_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_average_rating_deviation(dataset):\n",
    "    user_average_rating_deviation = []\n",
    "    for data_point in dataset:\n",
    "        if data_point['reviewerID'] in train_user_mean_rating_deviation:\n",
    "            user_average_rating_deviation.append(train_user_mean_rating_deviation[data_point['reviewerID']])\n",
    "        else:\n",
    "            user_average_rating_deviation.append(train_global_average_ratings - data_point['rating'])\n",
    "    return user_average_rating_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_review_stopwords(dataset):\n",
    "    review_stopwords = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for data_point in dataset:\n",
    "        review = data_point['reviewText'].lower().split()\n",
    "        review_stopwords.append(np.log(sum([1 if word in stop_words else 0 for word in review])+1))\n",
    "    return review_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_review_non_stopwords(dataset):\n",
    "    review_nonstopwords = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for data_point in dataset:\n",
    "        review = data_point['reviewText'].lower().split()\n",
    "        review_nonstopwords.append(np.log(sum([1 if word not in stop_words else 0 for word in review])+1))\n",
    "    return review_nonstopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_review_positive_words(dataset):\n",
    "    review_positive_words = []\n",
    "    for data_point in dataset:\n",
    "        review = data_point['reviewText'].lower().split()\n",
    "        review_positive_words.append(np.log(sum([1 if word in positive_words else 0 for word in review])+1))\n",
    "    return review_positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_review_negative_words(dataset):\n",
    "    review_negative_words = []\n",
    "    for data_point in dataset:\n",
    "        review = data_point['reviewText'].lower().split()\n",
    "        review_negative_words.append(np.log(sum([1 if word in negative_words else 0 for word in review])+1))\n",
    "    return review_negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_review_posneg_diff_words(dataset):\n",
    "    review_posneg_diff_words = []\n",
    "    for data_point in dataset:\n",
    "        review = data_point['reviewText'].lower().split()\n",
    "        neg = sum([1 if word in negative_words else 0 for word in review])\n",
    "        pos = sum([1 if word in positive_words else 0 for word in review])\n",
    "        review_posneg_diff_words.append(np.log(abs(neg-pos)+1))\n",
    "    return review_posneg_diff_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_summary_positive_words(dataset):\n",
    "    review_positive_words = []\n",
    "    for data_point in dataset:\n",
    "        review = data_point['summary'].lower().split()\n",
    "        review_positive_words.append(np.log(sum([1 if word in positive_words else 0 for word in review])+1))\n",
    "    return review_positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_summary_negative_words(dataset):\n",
    "    review_negative_words = []\n",
    "    for data_point in dataset:\n",
    "        review = data_point['summary'].lower().split()\n",
    "        review_negative_words.append(np.log(sum([1 if word in negative_words else 0 for word in review])+1))\n",
    "    return review_negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_summary_posneg_diff_words(dataset):\n",
    "    review_posneg_diff_words = []\n",
    "    for data_point in dataset:\n",
    "        review = data_point['summary'].lower().split()\n",
    "        neg = sum([1 if word in negative_words else 0 for word in review])\n",
    "        pos = sum([1 if word in positive_words else 0 for word in review])\n",
    "        review_posneg_diff_words.append(np.log(abs(neg-pos)+1))\n",
    "    return review_posneg_diff_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_review_season(dataset):\n",
    "    nov_dec_jan = []\n",
    "    feb_mar_apr = []\n",
    "    may_jun_jul = []\n",
    "    aug_sep_oct = []\n",
    "    \n",
    "    for data_point in dataset:\n",
    "        month = int((data_point['reviewTime'].split(',')[0]).split()[0])\n",
    "        if month == 1 or month > 10:\n",
    "            nov_dec_jan.append(1)\n",
    "            feb_mar_apr.append(0)\n",
    "            may_jun_jul.append(0)\n",
    "            aug_sep_oct.append(0)\n",
    "        if month >= 2 and month <= 4 :\n",
    "            nov_dec_jan.append(0)\n",
    "            feb_mar_apr.append(1)\n",
    "            may_jun_jul.append(0)\n",
    "            aug_sep_oct.append(0)\n",
    "        if month >= 5 and month <= 7 :\n",
    "            nov_dec_jan.append(0)\n",
    "            feb_mar_apr.append(0)\n",
    "            may_jun_jul.append(1)\n",
    "            aug_sep_oct.append(0)\n",
    "        if month >= 8 and month <= 10 :\n",
    "            nov_dec_jan.append(0)\n",
    "            feb_mar_apr.append(0)\n",
    "            may_jun_jul.append(0)\n",
    "            aug_sep_oct.append(1)\n",
    "\n",
    "    return nov_dec_jan, feb_mar_apr, may_jun_jul, aug_sep_oct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_review_month(dataset):\n",
    "    review_months=[[0]*len(dataset) for n in range(12)] \n",
    "    i=0\n",
    "    for data_point in dataset:\n",
    "        month = int((data_point['reviewTime'].split(',')[0]).split()[0])\n",
    "        review_months[month-1][i]=1\n",
    "        i+=1\n",
    "    return review_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_item_review_count(dataset):\n",
    "    item_review_count = []\n",
    "    for data_point in dataset:\n",
    "        if data_point['itemID'] in train_item_review_count:\n",
    "            item_review_count.append(train_item_review_count[data_point['itemID']])\n",
    "        else:\n",
    "            item_review_count.append(0)\n",
    "    return item_review_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_rating_category(dataset):\n",
    "    bad = []\n",
    "    ok = []\n",
    "    good = []\n",
    "    \n",
    "    for data_point in dataset:\n",
    "        r = data_point['rating']\n",
    "        if r < 3.0:\n",
    "            bad.append(1)\n",
    "            ok.append(0)\n",
    "            good.append(0)\n",
    "        if r >=3.0 and r<5.0:\n",
    "            bad.append(0)\n",
    "            ok.append(1)\n",
    "            good.append(0)\n",
    "        if r == 5.0:\n",
    "            bad.append(0)\n",
    "            ok.append(0)\n",
    "            good.append(1)\n",
    "    return bad, ok, good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_outof_group(dataset):\n",
    "    outof_low = []\n",
    "    outof_mid1 = []\n",
    "    outof_mid2 = []\n",
    "    outof_high = []\n",
    "    for data_point in dataset:\n",
    "        out_of = (data_point['helpful'])['outOf']\n",
    "        if out_of < 10:\n",
    "            outof_low.append(1)\n",
    "            outof_mid1.append(0)\n",
    "            outof_mid2.append(0)\n",
    "            outof_high.append(0)\n",
    "            \n",
    "        elif out_of < 50:\n",
    "            outof_low.append(0)\n",
    "            outof_mid1.append(2)\n",
    "            outof_mid2.append(0)\n",
    "            outof_high.append(0)\n",
    "        \n",
    "        elif out_of < 140:\n",
    "            outof_low.append(0)\n",
    "            outof_mid1.append(0)\n",
    "            outof_mid2.append(3)\n",
    "            outof_high.append(0)\n",
    "            \n",
    "        elif out_of < 600:\n",
    "            outof_low.append(0)\n",
    "            outof_mid1.append(0)\n",
    "            outof_mid2.append(0)\n",
    "            outof_high.append(4)\n",
    "            \n",
    "    return outof_low, outof_mid1, outof_mid2, outof_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unixtime(dataset):\n",
    "    unixTime = []\n",
    "    user_unix_time = []\n",
    "    for data_point in dataset:\n",
    "        unixTime.append(data_point['unixReviewTime'])\n",
    "    max_unix = max(unixTime)\n",
    "    min_unix = min(unixTime)\n",
    "    \n",
    "    for data_point in dataset:\n",
    "        user_unix_time.append(np.log(max_unix - data_point['unixReviewTime'] + 1))\n",
    "    return unixTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_price_range(dataset):\n",
    "    cheap = []\n",
    "    low = []\n",
    "    mid = []\n",
    "    high = []\n",
    "    for data_point in dataset:\n",
    "        if 'price' in data_point:\n",
    "            if data_point['price'] <= 10.:\n",
    "                cheap.append(1)\n",
    "                low.append(0)\n",
    "                mid.append(0)\n",
    "                high.append(0)\n",
    "            elif data_point['price'] <= 50.:\n",
    "                cheap.append(0)\n",
    "                low.append(1)\n",
    "                mid.append(0)\n",
    "                high.append(0)\n",
    "            elif data_point['price'] <= 150.:\n",
    "                cheap.append(0)\n",
    "                low.append(0)\n",
    "                mid.append(1)\n",
    "                high.append(0)\n",
    "            elif data_point['price'] > 150.:\n",
    "                cheap.append(0)\n",
    "                low.append(0)\n",
    "                mid.append(0)\n",
    "                high.append(1)\n",
    "        else:\n",
    "            cheap.append(0)\n",
    "            low.append(0)\n",
    "            mid.append(0)\n",
    "            high.append(0)\n",
    "    return cheap,low,mid,high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpfulness Prediction Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training Feature Set for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features(dataset):\n",
    "\n",
    "    ratings = get_rating(dataset)\n",
    "    print \"Ratings extracted..\" + str(np.matrix(ratings).shape)\n",
    "\n",
    "    price = get_price(dataset)\n",
    "    print \"Price feature extracted.. \" + str(np.matrix(price).shape)\n",
    "\n",
    "    square_ratings = get_square_rating(dataset)\n",
    "    print \"Squared Ratings extracted.. \" + str(np.matrix(square_ratings).shape)\n",
    "\n",
    "    total_helpfulness_votes = get_helpfulness_votes(dataset)\n",
    "    print \"Helpfulness extracted..\"+ str(np.matrix(total_helpfulness_votes).shape)\n",
    "\n",
    "    review_word_count = get_review_word_count(dataset)\n",
    "    print \"Review word count extracted..\"+ str(np.matrix(review_word_count).shape)\n",
    "    \n",
    "    review_sentence_count = get_sentence_count(dataset)\n",
    "    print \"Review Sentence count extracted..\"+ str(np.matrix(review_sentence_count).shape)\n",
    "    \n",
    "    review_word_allcaps_count = get_review_allcaps_count(dataset)\n",
    "    print \"Review word all caps extracted..\"+ str(np.matrix(review_word_allcaps_count).shape)\n",
    "    \n",
    "    review_char_count = get_review_char_count(dataset)\n",
    "    print \"Review character count extracted..\"+ str(np.matrix(review_char_count).shape)\n",
    "\n",
    "    item_rating_deviation = get_item_rating_deviation(dataset)\n",
    "    print \"Item rating deviation extracted..\"+ str(np.matrix(item_rating_deviation).shape)\n",
    "    \n",
    "    summary_word_count = get_summary_word_count(dataset)\n",
    "    print \"Summary word count extracted..\"+ str(np.matrix(summary_word_count).shape)\n",
    "\n",
    "    summary_word_allcaps_count = get_summary_allcaps_count(dataset)\n",
    "    print \"Summary word all caps extracted..\"+ str(np.matrix(summary_word_allcaps_count).shape)\n",
    "\n",
    "    user_review_experience = get_user_review_experience(dataset)\n",
    "    print \"User review experience extracted..\" + str(np.matrix(user_review_experience).shape)\n",
    "    \n",
    "    category_0, category_1, category_2, category_3, category_4 = get_category_id(dataset)\n",
    "    print \"One hot encoding of category ID complete..\" + str(np.matrix(category_0).shape)\n",
    "\n",
    "    review_readability_index = get_readability_index(dataset)\n",
    "    print \"Review readability score extracted..\"+ str(np.matrix(review_readability_index).shape)\n",
    "    \n",
    "    review_posneg_diff = get_review_posneg_diff_words(dataset)\n",
    "    print \"Review positive-negative difference extracted..\" + str(np.matrix(review_posneg_diff).shape)\n",
    "    \n",
    "    review_stopwords = get_review_stopwords(dataset)\n",
    "    print \"Review stop words extracted.. \" + str(np.matrix(review_stopwords).shape)\n",
    "    \n",
    "    summary_neg_words = get_summary_negative_words(dataset)\n",
    "    print \"Summary negative words extracted..\" + str(np.matrix(summary_neg_words).shape)\n",
    "\n",
    "    summary_specialchar_count = get_summary_specialchar_count(dataset)\n",
    "    print \"Summary special character count extracted..\"+ str(np.matrix(summary_specialchar_count).shape)\n",
    "    \n",
    "    man_cat, woman_cat, boy_cat, girl_cat = get_tag_category(dataset)\n",
    "    print \"Extracted audience categories..\"\n",
    "        \n",
    "    summary_pos_words = get_summary_positive_words(dataset)\n",
    "    print \"Summary positive words extracted..\" + str(np.matrix(summary_pos_words).shape)\n",
    "      \n",
    "    summary_posneg_words = get_summary_posneg_diff_words(dataset)\n",
    "    print \"Summary posneg difference extracted.. \" + str(np.matrix(summary_posneg_words).shape)\n",
    "\n",
    "    rating_bad, rating_ok, rating_good = get_rating_category(dataset)\n",
    "    print \"Extracted rating category..\" + str(np.matrix(rating_bad).shape)\n",
    "\n",
    "    review_nonstopwords = get_review_non_stopwords(dataset)\n",
    "    print \"Review non-stop words extracted..\" + str(np.matrix(review_nonstopwords).shape)\n",
    "    \n",
    "    review_year = get_review_year(dataset)\n",
    "    print \"Review years extracted..\" + str(np.matrix(review_year).shape)\n",
    "    \n",
    "    outOf_low, outOf_mid1, outOf_mid2, outOf_high = get_outof_group(dataset)\n",
    "    print \"Extracted one-hot encoded outOf categories..\" + str(np.matrix(outOf_low).shape)\n",
    "    \n",
    "    unixTime = get_unixtime(dataset)\n",
    "    print \"Extracted unix time of review..\" + str(np.matrix(unixTime).shape)\n",
    "    \n",
    "    price_cheap, price_low, price_mid, price_high = get_price_range(dataset)\n",
    "    print \"Extracted one-hot encoded price ranges..\" + str(np.matrix(price_cheap).shape)\n",
    "    \n",
    "##################################################################################################################\n",
    "    \n",
    "#     test_item_review_count = get_item_review_count(dataset)\n",
    "#     print \"Extracted item review count ..\" + str(np.matrix(test_item_review_count).shape)\n",
    "    \n",
    "#     test_adult_cat, test_kid_cat = get_adult_category(dataset)\n",
    "#     print \"Extracted age category..\"\n",
    "    \n",
    "#     test_male_cat, test_female_cat = get_sex_category(dataset)\n",
    "#     print \"Extract sex category..\"\n",
    "     \n",
    "#     test_review_pos_words = get_review_positive_words(dataset)\n",
    "#     print \"Review positive words extracted..\" + str(np.matrix(test_review_pos_words).shape)\n",
    "    \n",
    "#     test_review_neg_words = get_review_negative_words(dataset)\n",
    "#     print \"Review negative words extracted..\" + str(np.matrix(test_review_neg_words).shape)\n",
    "  \n",
    "#     test_average_helpfulness = get_average_helpfulness(dataset)\n",
    "#     print \"Average Helfulness extracted.. \" + str(np.matrix(test_average_helpfulness).shape)\n",
    "    \n",
    "#     test_user_avg_ratings = get_user_average_ratings(dataset)\n",
    "#     print \"User average ratings extracted.. \" + str(np.matrix(test_user_avg_ratings).shape)\n",
    "    \n",
    "#     test_user_avg_rating_deviation = get_user_average_rating_deviation(dataset)\n",
    "#     print \"User average rating deviation extracted.. \" + str(np.matrix(test_user_avg_rating_deviation).shape)\n",
    "    \n",
    "#     test_data_log_ratings = get_log_ratings(dataset)\n",
    "#     print \"Log Ratings extracted.. \" + str(np.matrix(test_data_log_ratings).shape)\n",
    "    \n",
    "#     test_data_review_specialchar_count = get_review_specialchar_count(dataset)\n",
    "#     print \"Review special character extracted..\"+ str(np.matrix(test_data_review_specialchar_count).shape)\n",
    "    \n",
    "#     test_data_review_flesch_reading_score = get_flesch_reading_ease_score(dataset)\n",
    "#     print \"Review flesch reading score extracted..\"+ str(np.matrix(test_data_review_flesch_reading_score).shape)\n",
    "        \n",
    "#     test_data_summary_char_count = get_summary_char_count(dataset)\n",
    "#     print \"Summary character count extracted\"+ str(np.matrix(test_data_summary_char_count).shape)\n",
    "       \n",
    "#     test_data_pos_sentiment_score, test_data_neg_sentiment_score, test_data_obj_sentiment_score = get_review_sentiment_score(dataset)\n",
    "#     print \"Review Sentiment scores extracted..\"\n",
    "    \n",
    "#     test_data_summ_pos_sentiment_score, test_data_summ_neg_sentiment_score, \\\n",
    "#         test_data_summ_obj_sentiment_score = get_summary_sentiment_score(dataset)\n",
    "#     print \"Summary sentiment scores extracted..\"\n",
    "\n",
    "#     review_months = get_review_month(dataset)\n",
    "#     print \"Extracted encoded months..\" \n",
    "######################################################################################################################    \n",
    "  \n",
    "    feature_set = [\n",
    "        np.ones(len(dataset)),\n",
    "        ratings,\n",
    "        price,\n",
    "        square_ratings,\n",
    "        total_helpfulness_votes,\n",
    "        review_word_count,\n",
    "        review_sentence_count,\n",
    "        review_word_allcaps_count,\n",
    "        review_char_count,\n",
    "        item_rating_deviation,\n",
    "        summary_word_count,\n",
    "        summary_word_allcaps_count,\n",
    "        user_review_experience, \n",
    "        category_0, \n",
    "        category_1, \n",
    "        category_2, \n",
    "        category_3, \n",
    "        category_4,\n",
    "        review_readability_index,\n",
    "        review_posneg_diff,\n",
    "        review_stopwords,\n",
    "        summary_neg_words,\n",
    "        summary_specialchar_count,\n",
    "        man_cat,\n",
    "        woman_cat,\n",
    "        boy_cat,\n",
    "        girl_cat,\n",
    "        outOf_low,\n",
    "        outOf_mid1,\n",
    "        outOf_mid2,\n",
    "        outOf_high,\n",
    "        rating_bad, \n",
    "        rating_ok, \n",
    "        rating_good,\n",
    "        unixTime,\n",
    "        price_cheap,\n",
    "        price_low,\n",
    "        price_mid,\n",
    "        price_high,\n",
    "        \n",
    "        #########################################\n",
    "#         test_item_review_count,\n",
    "#         test_review_month,\n",
    "#         test_adult_cat,\n",
    "#         test_kid_cat,\n",
    "#         test_male_cat,\n",
    "#         test_female_cat,\n",
    "#         test_summary_pos_words,\n",
    "#         test_summary_posneg_words,\n",
    "#         test_review_pos_words,\n",
    "#         test_review_neg_words,        \n",
    "#         test_average_helpfulness,\n",
    "#         test_user_avg_ratings,\n",
    "#         test_data_log_ratings,\n",
    "#         test_data_review_specialchar_count,\n",
    "#         test_user_avg_rating_deviation,\n",
    "#         test_data_review_flesch_reading_score,\n",
    "#         test_data_summary_char_count,   \n",
    "#         test_data_summ_pos_sentiment_score,\n",
    "#         test_data_summ_neg_sentiment_score,\n",
    "#         test_data_summ_obj_sentiment_score,\n",
    "#         test_review_year,\n",
    "#         test_review_nonstopwords,\n",
    "    ]\n",
    "\n",
    "    dataset = np.stack(feature_set, axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings extracted..(1L, 4621L)\n",
      "Price feature extracted.. (1L, 4621L)\n",
      "Squared Ratings extracted.. (1L, 4621L)\n",
      "Helpfulness extracted..(1L, 4621L)\n",
      "Review word count extracted..(1L, 4621L)\n",
      "Review Sentence count extracted..(1L, 4621L)\n",
      "Review word all caps extracted..(1L, 4621L)\n",
      "Review character count extracted..(1L, 4621L)\n",
      "Item rating deviation extracted..(1L, 4621L)\n",
      "Summary word count extracted..(1L, 4621L)\n",
      "Summary word all caps extracted..(1L, 4621L)\n",
      "User review experience extracted..(1L, 4621L)\n",
      "One hot encoding of category ID complete..(1L, 4621L)\n",
      "Review readability score extracted..(1L, 4621L)\n",
      "Review positive-negative difference extracted..(1L, 4621L)\n",
      "Review stop words extracted.. (1L, 4621L)\n",
      "Summary negative words extracted..(1L, 4621L)\n",
      "Summary special character count extracted..(1L, 4621L)\n",
      "Extracted audience categories..\n",
      "Summary positive words extracted..(1L, 4621L)\n",
      "Summary posneg difference extracted.. (1L, 4621L)\n",
      "Extracted rating category..(1L, 4621L)\n",
      "Review non-stop words extracted..(1L, 4621L)\n",
      "Review years extracted..(1L, 4621L)\n",
      "Extracted one-hot encoded outOf categories..(1L, 4621L)\n",
      "Extracted unix time of review..(1L, 4621L)\n",
      "Extracted one-hot encoded price ranges..(1L, 4621L)\n"
     ]
    }
   ],
   "source": [
    "train_high_dataset = get_features(high_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings extracted..(1L, 34300L)\n",
      "Price feature extracted.. (1L, 34300L)\n",
      "Squared Ratings extracted.. (1L, 34300L)\n",
      "Helpfulness extracted..(1L, 34300L)\n",
      "Review word count extracted..(1L, 34300L)\n",
      "Review Sentence count extracted..(1L, 34300L)\n",
      "Review word all caps extracted..(1L, 34300L)\n",
      "Review character count extracted..(1L, 34300L)\n",
      "Item rating deviation extracted..(1L, 34300L)\n",
      "Summary word count extracted..(1L, 34300L)\n",
      "Summary word all caps extracted..(1L, 34300L)\n",
      "User review experience extracted..(1L, 34300L)\n",
      "One hot encoding of category ID complete..(1L, 34300L)\n",
      "Review readability score extracted..(1L, 34300L)\n",
      "Review positive-negative difference extracted..(1L, 34300L)\n",
      "Review stop words extracted.. (1L, 34300L)\n",
      "Summary negative words extracted..(1L, 34300L)\n",
      "Summary special character count extracted..(1L, 34300L)\n",
      "Extracted audience categories..\n",
      "Summary positive words extracted..(1L, 34300L)\n",
      "Summary posneg difference extracted.. (1L, 34300L)\n",
      "Extracted rating category..(1L, 34300L)\n",
      "Review non-stop words extracted..(1L, 34300L)\n",
      "Review years extracted..(1L, 34300L)\n",
      "Extracted one-hot encoded outOf categories..(1L, 34300L)\n",
      "Extracted unix time of review..(1L, 34300L)\n",
      "Extracted one-hot encoded price ranges..(1L, 34300L)\n"
     ]
    }
   ],
   "source": [
    "train_low_dataset = get_features(low_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted helpfulness score for 4621 data points\n"
     ]
    }
   ],
   "source": [
    "train_high_helpfulness = []\n",
    "for data_point in high_dataset:\n",
    "    data_helpfulness = data_point['helpful']\n",
    "    if data_helpfulness['outOf'] > 10:\n",
    "        train_high_helpfulness.append(data_helpfulness['nHelpful'] * 1.0/data_helpfulness['outOf'])\n",
    "\n",
    "train_high_helpfulness = np.matrix(train_high_helpfulness).T\n",
    "print \"Extracted helpfulness score for \" + str(len(train_high_helpfulness)) + \" data points\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted helpfulness score for 34300 data points\n"
     ]
    }
   ],
   "source": [
    "train_low_helpfulness = []\n",
    "for data_point in low_dataset:\n",
    "    data_helpfulness = data_point['helpful']\n",
    "    train_low_helpfulness.append(data_helpfulness['nHelpful'] * 1.0/data_helpfulness['outOf'])\n",
    "\n",
    "train_low_helpfulness = np.matrix(train_low_helpfulness).T\n",
    "print \"Extracted helpfulness score for \" + str(len(train_low_helpfulness)) + \" data points\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2310L, 39L)\n",
      "(2311L, 39L)\n",
      "(2310L, 1L)\n",
      "(2311L, 1L)\n"
     ]
    }
   ],
   "source": [
    "train_high_x = train_high_dataset[:int(0.5*len(train_high_dataset))]\n",
    "valid_high_x = train_high_dataset[int(0.5*len(train_high_dataset)):]\n",
    "train_high_y = train_high_helpfulness[:int(0.5*len(train_high_helpfulness))]\n",
    "valid_high_y = train_high_helpfulness[int(0.5*len(train_high_helpfulness)):]\n",
    "print train_high_x.shape\n",
    "print valid_high_x.shape\n",
    "print train_high_y.shape\n",
    "print valid_high_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17150L, 39L)\n",
      "(17150L, 39L)\n",
      "(17150L, 1L)\n",
      "(17150L, 1L)\n"
     ]
    }
   ],
   "source": [
    "train_low_x = train_low_dataset[:int(0.5*len(train_low_dataset))]\n",
    "valid_low_x = train_low_dataset[int(0.5*len(train_low_dataset)):]\n",
    "train_low_y = train_low_helpfulness[:int(0.5*len(train_low_helpfulness))]\n",
    "valid_low_y = train_low_helpfulness[int(0.5*len(train_low_helpfulness)):]\n",
    "print train_low_x.shape\n",
    "print valid_low_x.shape\n",
    "print train_low_y.shape\n",
    "print valid_low_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ElasticNet Regressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "predictor_high = ElasticNet(alpha=0.09, l1_ratio=0.005)\n",
    "predictor_high.fit((train_high_x), (train_high_y))\n",
    "predict_high_y = predictor_high.predict((valid_high_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'n_estimators': 220, 'max_depth': 4, 'min_samples_split': 2, 'loss': 'ls'}\n",
    "predictor_low = ensemble.GradientBoostingRegressor(**params)\n",
    "predictor_low.fit((train_low_x), (train_low_y))\n",
    "predict_low_y = predictor_low.predict((valid_low_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # # Linear Regression Model\n",
    "# predictor = linear_model.LinearRegression()\n",
    "# predictor_low.fit((train_low_x), (train_low_y))\n",
    "# predict_low_y = predictor_low.predict((valid_low_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of Predictor : 0.0832317539593\n"
     ]
    }
   ],
   "source": [
    "# Mean Absolute Error\n",
    "mae_high = skmetrics.mean_absolute_error(valid_high_y, predict_high_y)\n",
    "print \"Mean Absolute Error of Predictor : \" + str(mae_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of Predictor : 0.189705696104\n"
     ]
    }
   ],
   "source": [
    "# Mean Absolute Error\n",
    "mae_low = skmetrics.mean_absolute_error(valid_low_y, predict_low_y)\n",
    "print \"Mean Absolute Error of Predictor : \" + str(mae_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Complete Dataset for Test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=0.09, copy_X=True, fit_intercept=True, l1_ratio=0.005,\n",
       "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN Linear Regression Model\n",
    "predictor = ElasticNet(alpha=0.09, l1_ratio=0.005)\n",
    "predictor.fit((train_high_dataset), (train_high_helpfulness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=4, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=150,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'n_estimators': 150, 'max_depth': 4, 'min_samples_split': 2, 'loss': 'ls'}\n",
    "predictor_low = ensemble.GradientBoostingRegressor(**params)\n",
    "predictor_low.fit((train_low_dataset),(train_low_helpfulness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_predict(m_predictor, x_test):\n",
    "    return m_predictor.predict(np.matrix(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "for line in read_gzip(\"test_Helpful.json.gz\"):\n",
    "    test_dataset.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings extracted..(1L, 14000L)\n",
      "Price feature extracted.. (1L, 14000L)\n",
      "Squared Ratings extracted.. (1L, 14000L)\n",
      "Helpfulness extracted..(1L, 14000L)\n",
      "Review word count extracted..(1L, 14000L)\n",
      "Review Sentence count extracted..(1L, 14000L)\n",
      "Review word all caps extracted..(1L, 14000L)\n",
      "Review character count extracted..(1L, 14000L)\n",
      "Item rating deviation extracted..(1L, 14000L)\n",
      "Summary word count extracted..(1L, 14000L)\n",
      "Summary word all caps extracted..(1L, 14000L)\n",
      "User review experience extracted..(1L, 14000L)\n",
      "One hot encoding of category ID complete..(1L, 14000L)\n",
      "Review readability score extracted..(1L, 14000L)\n",
      "Review positive-negative difference extracted..(1L, 14000L)\n",
      "Review stop words extracted.. (1L, 14000L)\n",
      "Summary negative words extracted..(1L, 14000L)\n",
      "Summary special character count extracted..(1L, 14000L)\n",
      "Extracted audience categories..\n",
      "Summary positive words extracted..(1L, 14000L)\n",
      "Summary posneg difference extracted.. (1L, 14000L)\n",
      "Extracted rating category..(1L, 14000L)\n",
      "Review non-stop words extracted..(1L, 14000L)\n",
      "Review years extracted..(1L, 14000L)\n",
      "Extracted one-hot encoded outOf categories..(1L, 14000L)\n",
      "Extracted unix time of review..(1L, 14000L)\n",
      "Extracted one-hot encoded price ranges..(1L, 14000L)\n",
      "(14000L, 39L)\n"
     ]
    }
   ],
   "source": [
    "test_feature_set = get_features(test_dataset)\n",
    "print test_feature_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare results file for KAGGLE UPLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Helpful.txt\", 'w')\n",
    "idx = 0\n",
    "for l in open(\"pairs_Helpful.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,i,outOf = l.strip().split('-')\n",
    "    outOf = int(outOf)\n",
    "    if outOf > 10:\n",
    "        pred = int(round(outOf*test_predict(predictor, test_feature_set[idx])))\n",
    "    else:\n",
    "        pred = int(round(outOf*test_predict(predictor_low, test_feature_set[idx])))\n",
    "    predictions.write(u + '-' + i + '-' + str(outOf) + ',' + str(pred) + '\\n')\n",
    "    idx += 1\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "====================================================================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
